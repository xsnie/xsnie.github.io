---
layout: paper
categories: papers
permalink: papers/scopevit
id: scopevit
title: "ScopeViT: Scale-Aware Vision Transformer"
feature-title: ScopeViT
authors: 
  - Xuesong Nie
  - Haoyuan Jin
  - Yunfeng Yan
  - Xi Chen
  - Zhihang Zhu
  - Donglian Qi
first-author: true
venue: Pattern Recognition
venue-shorthand: PR
year: 2024
url: /papers/scopevit
pdf: https://arxiv.org/abs/2310.04621
code: https://github.com/xuesongnie
type: journal
figure: /images/papers/scopevit.png
doi: 10.1145/3491102.3501823
selected: true
feature-description: ScopeViT empowers Transformers with scale-aware capabilities to learn visual information
image: /images/featured/scopevit.png
featured: true
feature-order: 2
coming-soon: false
bibtex: |-

    @inproceedings{nie2024scopevit,
        title={ScopeViT: Scale-Aware Vision Transformer},
        author={Xuesong Nie and Haoyuan Jin and Yunfeng Yan and Xi Chen and Zhihang Zhu and Donglian Qi},
        booktitle={Pattern Recognition (PR)},
        year={2024}
    }
---

Multi-scale features are essential for various vision tasks, such as classification, detection, and segmentation. 
Although Vision Transformers (ViTs) show remarkable success in capturing global features within an image, how to leverage multi-scale features in Transformers is not well explored. 
This paper proposes a scale-aware vision Transformer called ScopeViT that efficiently captures multi-granularity representations. 
Two novel attention with lightweight computation are introduced: Multi-Scale Self-Attention (MSSA) and Global-Scale Dilated Attention (GSDA). 
MSSA embeds visual tokens with different receptive fields into distinct attention heads, allowing the model to perceive various scales across the network. 
GSDA enhances model understanding of the global context through token-dilation operation, which reduces the number of tokens involved in attention computations. 
This dual attention method enables ScopeViT to "see" various scales throughout the entire network and effectively learn inter-object relationships, reducing heavy quadratic computational complexity. 
Extensive experiments demonstrate that ScopeViT achieves competitive complexity/accuracy trade-offs compared to existing networks across a wide range of visual tasks. 
On the ImageNet-1K dataset, ScopeViT achieves a top-1 accuracy of 81.1%, using only 7.4M parameters and 2.0G FLOPs. 
Our approach outperforms Swin (ViT-based) by 1.9% accuracy while saving 42% of the parameters, outperforms MobileViTv2 (Hybrid-based) with a 0.7% accuracy gain while using 50% of the computations, and also beats ConvNeXt V2 (ConvNet-based) by 0.8% with fewer parameters.