---
layout: paper
categories: papers
permalink: papers/samp
id: samp
title: "SAMP: Adapting Segment Anything Model for Pose Estimation"
feature-title: SAMP
authors: 
  - Zhihang Zhu
  - Yunfeng Yan
  - Yi Chen  
  - Haoyuan Jin
  - Xuesong Nie
  - Donglian Qi
  - Xi Chen
venue: IEEE Conference on Multimedia Expo
venue-shorthand: ICME
location: Niagra Falls, Canada
year: 2024
url: /papers/samp
pdf: https://arxiv.org/abs/2310.04621
code: https://github.com/xuesongnie
type: conference
figure: /images/papers/samp.png
doi: 10.1145/3491102.3501823
selected: true
feature-description: we propose Segment Anything Model for Pose, SAMP, which makes the first attempt to adapt SAM for pose estimation
image: /images/featured/samp.png
featured: false
dissertation: false
feature-order: 1
coming-soon: false
bibtex: |-

    @inproceedings{zhu2024samp,
        title={SAMP: Adapting Segment Anything Model for Pose Estimation},
        author={Zhihang Zhu and Yunfeng Yan and Yi Chen and Haoyuan Jin and Xuesong Nie and Donglian Qi and Xi Chen},
        booktitle={IEEE Conference on Multimedia Expo (ICME)},
        year={2024}
    }
---

Segment Anything Model (SAM) exhibits superior performance for segmentation. 
Many follow-up works explore adapting this powerful model to specific domains. 
However, those works mainly focus on different sub-tasks of segmentation. 
The cross-task generalization ability of SAM is still not explored. 
In this paper, we propose SAMP (SAM for Pose), which makes the first attempt to adapt SAM for pose estimation. 
We observe that SAM could segment different human parts with specific prompts, proving that it contains the knowledge to understand the human structure. 
Considering that localizing keypoints requires fine-grained perceptual capabilities, we design a Detail-aware Adapter (DA-Adapter), which complements the features of the SAM encoder with multi-scale feature fusion and multi-level supervision. 
Experimental results demonstrate that SAMP achieves novel state-of-the-art against previously specifically designed pose estimation methods. 
Specifically, with ViT-B backbone, SAMP achieves 78.1% AP on the COCO val2017, 77.1% AP on the COCO test-dev2017, and 70.5% AP on the CrowdPose dataset.