---
layout: paper
categories: papers
permalink: papers/tat
id: tat
title: "Triplet Attention Transformer for Spatiotemporal Predictive Learning"
feature-title: TAT
authors: 
  - Xuesong Nie
  - Xi Chen
  - Haoyuan Jin
  - Zhihang Zhu
  - Yunfeng Yan
  - Donglian Qi
venue: IEEE/CVF Winter Conference on Applications of Computer Vision
venue-shorthand: WACV
location: Waikoloa, Hawaii, USA
year: 2024
url: /papers/tat
pdf: https://arxiv.org/abs/2310.04621
code: https://github.com/xuesongnie
type: conference
figure: /images/papers/tat.png
doi: 10.1145/3491102.3501823
selected: true
feature-description: TAT replaces traditional recurrent units by exploring self-attention mechanisms in temporal, spatial, and channel dimensions <br><br> <b>Xuesong Nie</b>
image: /images/featured/tat.png
featured: false
dissertation: true
feature-order: 1
coming-soon: false
bibtex: |-

    @inproceedings{nie2024tat,
        title={Triplet Attention Transformer for Spatiotemporal Predictive Learning},
        author={Xuesong Nie and Xi Chen and Haoyuan Jin and Zhihang Zhu and Yunfeng Yan and Donglian Qi},
        booktitle={IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
        year={2024}
    }
---

Spatiotemporal predictive learning offers a self-supervised learning paradigm that enables models to learn both spatial and temporal patterns by predicting future sequences based on historical sequences. 
Mainstream methods are dominated by recurrent units, yet they are limited by their lack of parallelization and often underperform in real-world scenarios. 
To improve prediction quality while maintaining computational efficiency, we propose an innovative triplet attention transformer designed to capture both inter-frame dynamics and intra-frame static features. 
Specifically, the model incorporates the Triplet Attention Module (TAM), which replaces traditional recurrent units by exploring self-attention mechanisms in temporal, spatial, and channel dimensions. 
In this configuration: (i) temporal tokens contain abstract representations of inter-frame,  facilitating the capture of inherent temporal dependencies; 
(ii) spatial and channel attention combine to refine the intra-frame representation by performing fine-grained interactions across spatial and channel dimensions. 
Alternating temporal, spatial, and channel-level attention allows our approach to learn more complex short- and long-range spatiotemporal dependencies. 
Extensive experiments demonstrate performance surpassing existing recurrent-based and recurrent-free methods, achieving state-of-the-art under multi-scenario examination including moving object trajectory prediction, traffic flow prediction, driving scene prediction, and human motion capture.